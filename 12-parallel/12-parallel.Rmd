---
title: "Parallel programming"
author:
  name: Grant R. McDermott
  affiliation: University of Oregon | EC 607
  # email: grantmcd@uoregon.edu
date: Lecture 12  #"`r format(Sys.time(), '%d %B %Y')`"
output: 
  html_document:
    theme: flatly
    highlight: haddock 
    # code_folding: show
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dpi=300)
## Next hook based on this SO answer: https://stackoverflow.com/a/39025054
knitr::knit_hooks$set(
  prompt = function(before, options, envir) {
    options(
      prompt = if (options$engine %in% c('sh','bash')) '$ ' else 'R> ',
      continue = if (options$engine %in% c('sh','bash')) '$ ' else '+ '
      )
    })
```

*Note: This is the third of three lectures on programming. Please take a look at the [first](https://raw.githack.com/uo-ec607/lectures/master/10-funcs-intro/10-funcs-intro.html) [two](https://raw.githack.com/uo-ec607/lectures/master/11-funcs-adv/11-funcs-adv.html) lectures if you haven't yet. Nothing that we will cover here is critically dependent on these earlier lectures. However, I'm going to assume that you have a good understanding of how R functions and environments generally work. Our goal for today is to speed up our programming tasks by getting them to run in parallel.*

## Software requirements

### External software (optional)

Every operating system provides built-in tools for monitoring processes and resource useage. These typically come in the form of a nice GUI (e.g. [System Monitor](https://wiki.gnome.org/Apps/SystemMonitor)). However, I recommend taking a look at [**htop**](https://hisham.hm/htop/); a shell-based process viewer available for installation on Linux and Mac. Regardless of whether you install it now --- or can (sorry, Windows users) --- I wanted to flag `htop` before we get to the big data section of the course. At that point, we'll all be connecting to remote Linux servers through SSH, and a shell-based process monitor will prove very useful for tracking resource use and performance.

### R packages 

- **New:** `parallel`, `future`, `future.apply`, `furrr`, `RhpcBLASctl`
- **Already used:** `tidyverse`, `pbapply`, `tictoc`

Let's install (if necessary) and load all these packages now. Note that the `parallel` package is bundled together with the base R installation and should already be on your system. I'm also going to call the `future::plan()` function and set the resolution to "multiprocess". Don't worry what this means right now --- I'll explain in a bit --- just think of it as a convenient way to set the desired behaviour for the rest of this document.

```{r, cache=F, message=F}
## Load/install packages
if (!require("pacman")) install.packages("pacman")
pacman::p_load(tictoc, pbapply, future, future.apply, tidyverse, furrr, RhpcBLASctl)

## Set future::plan() resolution strategy
plan(multiprocess)
```

## Prologue

A few lectures back, we talked about the huge difference that some relatively new packages have made to spatial analysis in R. Complex spatial operations that previously necessitated equally complex spatial objects have been superseded by much simpler and more intuitive tools. If that wasn't good enough, these new tools are also faster. 

We are going to see something very similar today. Parallel programming is a big and complex topic, with many potential pitfalls. However, software innovations and some amazing new(ish) packages have made it *much* easier and safer to program in parallel.^[I should emphasise that the R-core team has provided excellent support for parallel programming for over a decade. But there's no question in my mind that the barriers to entry have recently been lowered.] 

With that in mind, I'm going to start today's lecture with a motivating example. My goal is to demonstrate both the ease and immediate payoff of "going parallel" in R. Then, after hopefully convincing you, I'll return to a more general discussion of parallel programming in general, highlight some technical concepts and discuss some remaining pitfalls that you should be aware of.

## Motivating example

Our motivating example is going to involve the same `slow_func()` function that we saw in the previous lecture:

```{r slow_func}
# library(tidyverse) ## Already loaded

## Emulate slow function
slow_func <- 
  function(x = 1) {
    x_sq <- x^2 
    df <- tibble(value=x, value_squared=x_sq)
    Sys.sleep(2)
    return(df)
    }
```

Let's iterate over this function using the standard `lapply()` method that we're all familar with by now. Note that this iteration will be executed in *serial*. I'll use the [tictoc package](https://cran.r-project.org/web/packages/tictoc/) to record timing.

```{r serial_ex}
# library(tictoc) ## Already loaded

tic()
serial_ex <- lapply(1:12, slow_func) %>% bind_rows()
toc()
```

Now, I'm going to iterate over the function in *parallel*. In my case, this means that I will be using all `r parallel::detectCores()` cores on my laptop. I'll use the [future.apply package](https://cran.r-project.org/web/packages/future.apply/index.html) (more about this later), but the parameters of the iteration are otherwise unchanged.

```{r future_ex}
# library(future.apply)  ## Already loaded
# plan(multiprocess) ## Already set above

tic()
future_ex <- future_lapply(1:12, slow_func) %>% bind_rows()
toc()
```

Wow, the execution time was twelve times faster! Even more impressively, look how little the syntax changed. I basically just had to tell R that I wanted to implement the iteration in parallel (`plan(multiprocess)`) and then add "future_" to my apply call (<code>**future_**apply()</code>). Let's confirm that the output is the same.

```{r all_equal_ex}
all_equal(serial_ex, future_ex)
```

Those of you who prefer the `purrr::map()` family of functions for iteration and are feeling left out... don't worry. The [furrr package](https://davisvaughan.github.io/furrr/index.html) (geddit?) has you covered. Once again, the syntax for these parallel functions will be very little changed from their serial implementations.^[We tell R that we want to run things in parallel with `plan(multiprocess)` and then call <code>**future_**map_df**r**()</code>. In this particular case, the extra "r" at the end tells future to concatenate the iterated frames by rows.]

```{r furrr_ex}
# library(furrr)  ## Already loaded
# plan(multiprocess) ## Already set above

tic()
furrr_ex <- future_map_dfr(1:12, slow_func)
toc()
```

How easy was that? I'm about to take a deeper dive into some of the technical issues that were abstracted away behind the scenes. But first congratulate yourself for already being such an expert at parallel programming.

<div align="center"><iframe src="https://giphy.com/embed/ujGfBmVppmgEg" width="480" height="359" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></p></div>
</br>

## Parallel basics

Motivating example out of the way, let's take a look underneath the hood. I'll start by clearing up some terminology.

### Terminology

- **Socket:** The physical connection on your computer that houses the processor. Most home and work computers --- even very high end ones --- only have one socket and thus processor. However, they can have multiple cores...
- **Core:** The part of the processor that actually performs the computation. In days gone by ~~when dinosaurs roamed the earth~~ processors were limited to a single core. However, modern processors typically house multiple cores. Each of these cores can perform entirely separate and independent computational tasks.
- **Process:** A single instance of a running task or program (R, Dropbox, etc). A core can only run one process at a time. However, it may give the appearance of doing more than that by efficiently scheduling between them. Speaking of which...
- **Thread:** A component or subset of a process that can, inter alia, share memory and resources with other threads. We'll return to this idea as it applies to *hyperthreading* in a few paragraphs.
- **Cluster:** A collection of objects that are capable of hosting cores. This could range from a single socket (on your home computer) to an array of servers (on a high-performance computing network).

You may wondering where the much-referenced **CPU** (i.e. central processing unit) fits into all of this. Truth be told, the meaning of CPU has evolved with the advent of new technology like multicore processors. For the purposes of this lecture I will use the following definition: 

$$\text{No. of CPUs} = \text{No. of sockets} \times \text{No. of cores} \times \text{No. of threads per core}$$

If nothing else, this is consistent with the way that Unix records information about CPU architecure via the [lscpu](https://linux.die.net/man/1/lscpu) command. For example, on my computer: 

```{bash lscpu, error=T, prompt=T}
## Run from bash-compatible shell
lscpu | grep -E '^Thread|^Core|^Socket|^CPU\('
```

### Look, just tell me how many parallel threads I can run

Okay, okay! Our abilty to go parallel hinges on the number of computing units available to us, i.e. the number of CPU cores. The simplest way to obtain this from R is with the `parallel::detectCores()` function:

```{r cache=F}
parallel::detectCores()
```

So I have `r parallel::detectCores()` cores available to me on this computer.^[A Dell Precision 5530 running Arch Linux, if you're interested.] The story is slightly complicated by the presence of so-called logical cores, though. These extend or emulate the ability of physical cores to perform additional tasks. The most famous example is [**hyperthreading**](https://en.wikipedia.org/wiki/Hyper-threading), an Intel technology that allows a single core to switch very rapidly between two different tasks. This mimics the appearance and performance (albeit to a lesser extent) of an extra physical core. You may find [this YouTube video](https://www.youtube.com/watch?v=mSZpDF-zUoI&) helpful for understanding the difference in more depth, including a nice analogy involving airport security lines.

Taking a step back, you don't have to worry too much about the difference between physical and logical (hyperthreaded) cores for the purpose of this lecture. R doesn't care whether you run a function on a physical core or a logical one. Both will work equally well. (Okay, the latter will be a little slower.) Still, if you are interested in determining the number of physical cores versus logical cores on your system then there are lots of ways to this.^[For example, on a Unix-based computer you can run `$ htop` (provided that you've [installed](https://hisham.hm/htop/) it) from the shell. Or you can run `$ lscpu`, with a fancier version available to those who remember the pipe operator from our shell lecture: `$ lscpu | grep -E '^Thread|^Core|^Socket|^CPU\('`.] A convenient way to get this information directly from R is with the [RhpcBLASctl package](https://cran.r-project.org/web/packages/RhpcBLASctl/index.html).

```{r cores_vs_procs}
# library(RhpcBLASctl) ## Already loaded

get_num_procs() ## No. of logical cores (including hyperthreading)
get_num_cores() ## No. of physical cores
```


### "Embarrassingly parallelizable" tasks

The crucial point is that these chunks of computation are unrelated and do not need to communicate in any way.


## Clusters

### FORK vs PSOCKS

To borrow from [Max Gordon](http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/#Fork_or_sock):

**FORK:** "to divide in branches and go separate ways"

- Systems: Unix/Mac (not Windows)
- Environment: Link all

**PSOCK:** Parallel Socket Cluster

- Systems: All (including Windows)
- Environment: Empty

While forking is faster and more memory efficient, it can cause problems in a GUI or IDE like RStudio. From the [parallel package vignette](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf):

> Note that as it does share the complete process, it also shares any GUI elements, for example an R console and on-screen devices. This can cause havoc.

I've only very rarely run into issues when running a forking process through RStudio.^[For example, when I'm running functions that contain many nested while-loops.] However, I want you to be aware of it, so that you aren't caught by surprise if it ever does happen to you. The simple solution is to run your R script from the terminal using, say `$ Rscript -e myscript.R`.

### Local vs remote

## Run R functions in parallel

We'll start by creating a fake dataset (`our_data`)

```{r reg_func}
## Set seed (for reproducibility)
set.seed(1234)
# Set sample size
n <- 1e6

## Generate a large data frame of fake data for a regression
our_data <- 
  tibble(x = rnorm(n), e = rnorm(n)) %>%
  mutate(y = 3 + 2*x + e)

## Function that draws a sample of 10,000 observations, runs a regression and extracts
## the coefficient value on the x variable (should be around 2).
reg_func <- 
  function(i) {
  ## Sample the data
  sample_data <- our_data[sample.int(n = n, size = 1e4, replace = T),]
  ## Run the regression on our sampled data and extract the extract the x coefficient.
  x_coef <- lm(y ~ x, data = sample_data)$coef[2]
  ## Return value
  return(x_coef)
  }
```


### Serial implementation (for comparison)

```{r sim_serial}
# library(tictoc) ## Already loaded

set.seed(123) ## Optional. Ensures results are exactly the same.

# 1,000-iteration simulation
tic()
sim_serial <- lapply(1:1e4, reg_func)
toc()
```

### Parallel implemention using the `future` package

#### 1) future.apply

```{r sim_future}
# library(future.apply) ## Already loaded
plan(multiprocess) 

set.seed(123) ## Optional. Ensures results are exactly the same.

# 1,000-iteration simulation
tic()
sim_future <- future_lapply(1:1e4, reg_func)
toc()
```

### 2) furrr

```{r sim_furrr}
# library(furrr) ## Already loaded
plan(multiprocess) 

set.seed(123) ## Optional. Ensures results are exactly the same.

# 1,000-iteration simulation
tic()
sim_furrr <- future_map(1:1e4, reg_func)
toc()
```

### Other parallel options

```{r sim_pblapply}
# library(pbapply) ## Already loaded

set.seed(123) ## Optional. Ensures results are exactly the same.

# 1,000-iteration simulation
tic()
sim_pblapply <- pblapply(1:1e4, reg_func, cl = parallel::detectCores())
toc()
```


## Explicit vs implicit parallelization

Thus far we have only been concerned with *explicit* parallelization. As in, we explicitly tell R to run a particular set of commands in parallel. But there is another form of *implicit* parallelization that is equally important to be aware of. In this case, certain low-level functions and operations are automatically run in parallel regardless of whether we told R to do so or not. Implicit parallelization can make a big difference to performance, but is not the default behaviour in R. So you have to enable it first. Moreover, combining explicit and implicit parallelization can cause problems if you don't take certain precautions.

### BLAS/LAPACK

Did you ever wonder how R and other programming languages perform their calculations? For example, how does R actually do things like vector addition, or scalar and matrix multiplication? The answer is [**BLAS**](http://www.netlib.org/blas/) (**B**asic **L**inear **A**lgebra **S**uprograms). BLAS are a collection of low-level routines that provide standard building blocks for performing basic vector and matrix operations. These routines are then incoporated in related libraries like [**LAPACK**](http://www.netlib.org/lapack/) (**L**inear **A**lgebra **Pack**age), which provide their own routines for solving systems of linear equations and linear least squares, calculating eigenvalues, etc. In other words, BLAS and LAPACK provide the linear algebra framework that supports virtually all of statistical and computational programming 

R ships with its own BLAS and LAPACK libraries by default. These libraries place a premium on stablility (e.g. common user experience across operating systems). While the default works well enough, you can get *significant* speedups by switching to more optimized libraries such as the [Intel Math Kernel Library (MKL)](https://software.intel.com/en-us/mkl) or [OpenBLAS](https://www.openblas.net/). Among other things, these optimised BLAS libraries support multi-threading. So now you are using all your available computer power to, say, solve a matrix.

You can use the `sessionInfo()` command to see which BLAS/LAPACK library you are using. For example, I am using OpenBLAS on this computer:

```{r blas_info}
sessionInfo()[c("BLAS", "LAPACK")]
```

### Beware resource competition

While this all sounds great --- and I certainly recommend taking a look at MKL or OpenBLAS --- there is a potential downside. In particular, you risk competing with yourself for computational resources (i.e. memory) if you mix explicit and implicit parallel calls. For instance, if you run explicit multicore functions from within R on a system that has been configured with an optimised BLAS. As [Dirk Eddelbuettel](http://dirk.eddelbuettel.com/) succintly puts it in [this SO answer](https://stackoverflow.com/a/18291826):

> There is one situation you want to avoid: (1) spreading a task over all N cores and (2) having each core work on the task using something like OpenBLAS or MKL with all cores. Because now you have an N by N contention: each of the N task wants to farm its linear algebra work out to all N cores.

Now, I want to emphasise that this conflict rarely matters in my own experience. I use explicit parallel calls all the time in my R scripts and have hardly ever run into a problem, despite my reliance on optimised BLAS libraries like MKL and OpenBLAS. Moreover, when these slowdowns have occured, I've found the effect to be relatively modest.^[The major cost appears to be the unnecessary duplication of objects in memory.] Still, I have read of cases where the effect can be quite dramatic (e.g. [here](https://stat.ethz.ch/pipermail/r-sig-hpc/2014-February/001846.html)) and so I wanted you to be aware of it all the same.

Luckily, there's also an easy and relatively costless solution: Simply turn off BLAS hyperthreading. It turns out this has minimal impact on performance, since most of the gains from an optimised BLAS library are actually You can turn off BLAS hyperthreading for the current R session via the `RhpcBLASctl::blas_set_num_threads()` function. For example, I sometimes include the following line at the top of an R script:
```{r eval=F}
# blas_get_num_procs() ## If you want to find the existing number of BLAS threads
blas_set_num_threads(1) ## Set BLAS threads to 1 (i.e. turn off multithreading)
```

Since this is only in effect for the current R session, BLAS multithreading will be restored when I restart R.^[You can turn off multithreading as the default by altering the configuration file when you first build/install your preferred BLAS library. However, that's both complicated and unecessarily restrictive in my view.] Of course, I can also just reinstate the original behaviour in the same session by running `blas_set_num_threads(parallel::detectCores())`.

## Other topics

### Overhead and Amdahl's law

### Fault tolerance

### Random number generation

### CPUs vs GPUs

Graphical Processing Units, or GPUs, are specialised chipsets that were originaly built to perform the heavy lifting associated with rendering graphics. It's important to realise that not all computers have GPUs. Most laptops come with so-called [integrated graphics](https://www.laptopmag.com/articles/intel-hd-graphics-comparison), which basically means that the same processor is performing both regular and graphic-rendering tasks. However, higher-end and gaming laptops (and many desktop computers) include a dedicated GPU card. For example, the Dell Precision 5530 that I'm writing these lecture notes on has a [hybrid graphics](https://wiki.archlinux.org/index.php/hybrid_graphics) setup with two cards: 1) an integrated Intel GPU (UHD 630) and 2) a discrete NVIDIA Quadro P2000.

So why am I telling you this? Well, it turns out that GPUs also excel at non-graphic computation tasks. The same processing power needed to perform the millions of parallel calculations for rendering 3-D games or architectural software, can be put to use on scientific problems. How exactly this was discovered involves an interesting backstory of supercomputers being built with Playstations. (Google it.) But the short version is that modern GPUs comprise *thousands* of cores that can be run in parallel. Or, as a colleague once memorably described it to me: "GPUs are basically just really, really good at doing linear algebra."

Still, that's about as much as I want to say about GPUs for now. Installing and maintaining a working GPU setup for scientific purposes is a much more complex task. (And, frankly, overkill for the vast majority of econometric or data science needs.) We may revisit the topic when we get to the machine learning section of the course in a few weeks.^[Advanced machine learning techniques like [deep learning](https://blog.rstudio.com/2018/09/12/getting-started-with-deep-learning-in-r/) are particularly performance-dependent on GPUs.] Thus, and while the general concepts carry over, everything that we've covered today is limited to CPUs.

### Parallel regression

`lfe`, `partools`, and various Bayesian packages (jags, stan, etc.)

### Monitoring multicore performance

htop

## Further resources

- The [parallel package vignette](https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf) provides a very good overview, not only its own purpose, but of parallel programming in general. Particular attention is paid to the steps needed to ensure a stable R environment (e.g. across operating systems).
